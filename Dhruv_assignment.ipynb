{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNavwidjp/jfnhMXywSasaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvpathak23/Assignment/blob/main/Dhruv_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxM7cP-wILn3",
        "outputId": "42bce3a8-ce11-4135-fcc1-491e6dd25e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Whisper model...\n",
            "Loaded in 0.9832863807678223 s\n",
            "Downloading audio...\n",
            "Analyzing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done in 15.300451517105103 s\n",
            "\n",
            "--- Results ---\n",
            "Talk-time ratio (A/B): {'A': 0.9999999988130563, 'B': 0.0}\n",
            "Number of questions: 16\n",
            "Longest monologue (s): 118.08\n",
            "Call sentiment: POSITIVE\n",
            "Actionable insight: Speaker A is dominating (~100.0%). Encourage the other speaker to participate more.\n",
            "Saved turns.csv\n"
          ]
        }
      ],
      "source": [
        "# Call Quality Analyzer - Colab-ready Python notebook\n",
        "\n",
        "# 1) Install required packages (Run in Colab)\n",
        "!pip install -q yt-dlp pydub transformers torch torchvision torchaudio librosa soundfile openai-whisper numpy scipy scikit-learn\n",
        "\n",
        "# Optional (pyannote) - only if you provide HF_TOKEN as an environment variable\n",
        "!pip install -q pyannote.audio\n",
        "\n",
        "# %%%\n",
        "# 2) Imports\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "from subprocess import run\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "from transformers import pipeline\n",
        "import whisper\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "\n",
        "# %%%\n",
        "# 3) Helper: download youtube audio and convert to mono WAV 16k\n",
        "YOUTUBE_URL = 'https://www.youtube.com/watch?v=4ostqJD3Psc'  # test file required by assignment\n",
        "OUT_WAV = 'call.wav'\n",
        "\n",
        "def download_audio(youtube_url, out_wav=OUT_WAV):\n",
        "    tmpfile = 'audio_temp.m4a'\n",
        "    run(['yt-dlp', '-x', '--audio-format', 'm4a', '-o', tmpfile, youtube_url], check=False)\n",
        "    audio = AudioSegment.from_file(tmpfile)\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "    audio.export(out_wav, format='wav')\n",
        "    return out_wav\n",
        "\n",
        "# %%%\n",
        "# 4) Voice activity detection (simple energy-based fallback)\n",
        "def get_speech_timestamps_energy(wav_path, frame_duration_ms=30, agg_energy_threshold=0.0005):\n",
        "    y, sr = librosa.load(wav_path, sr=16000)\n",
        "    frame_len = int(sr * (frame_duration_ms/1000.0))\n",
        "    frames = [y[i:i+frame_len] for i in range(0, len(y), frame_len)]\n",
        "    timestamps = []\n",
        "    is_speech = False\n",
        "    start = 0\n",
        "    for i, f in enumerate(frames):\n",
        "        energy = np.mean(f**2)\n",
        "        if energy > agg_energy_threshold and not is_speech:\n",
        "            is_speech = True\n",
        "            start = i*frame_duration_ms/1000.0\n",
        "        if energy <= agg_energy_threshold and is_speech:\n",
        "            end = i*frame_duration_ms/1000.0\n",
        "            timestamps.append((start, end))\n",
        "            is_speech = False\n",
        "    if is_speech:\n",
        "        timestamps.append((start, len(y)/sr))\n",
        "    return timestamps\n",
        "\n",
        "# %%%\n",
        "# 5) Load whisper model\n",
        "MODEL = 'tiny'  # tiny model for speed\n",
        "print('Loading Whisper model...')\n",
        "start = time.time()\n",
        "wh = whisper.load_model(MODEL)\n",
        "print('Loaded in', time.time()-start, 's')\n",
        "\n",
        "# %%%\n",
        "# 6) High-level pipeline function\n",
        "def analyze_call(wav_path):\n",
        "    vad_segs = get_speech_timestamps_energy(wav_path)\n",
        "    result = wh.transcribe(wav_path, word_timestamps=False)\n",
        "    segments = result.get('segments', [])\n",
        "\n",
        "    # Alternate speakers\n",
        "    turns = []\n",
        "    speaker = 'A'\n",
        "    last_end = None\n",
        "    for s in segments:\n",
        "        st, ed = s['start'], s['end']\n",
        "        txt = s['text'].strip()\n",
        "        if last_end is not None and (st - last_end) > 0.8:\n",
        "            speaker = 'B' if speaker=='A' else 'A'\n",
        "        turns.append((speaker, st, ed, txt))\n",
        "        last_end = ed\n",
        "\n",
        "    # Talk-time ratio (improved: total words per speaker weighted by duration)\n",
        "    dur_A = sum(ed-st for sp,st,ed,tx in turns if sp=='A')\n",
        "    dur_B = sum(ed-st for sp,st,ed,tx in turns if sp=='B')\n",
        "    words_A = sum(len(tx.split()) for sp,st,ed,tx in turns if sp=='A')\n",
        "    words_B = sum(len(tx.split()) for sp,st,ed,tx in turns if sp=='B')\n",
        "    total_time = dur_A + dur_B if (dur_A+dur_B)>0 else 1e-6\n",
        "    talk_ratio = {\n",
        "        'A': (0.6*(dur_A/total_time) + 0.4*(words_A/(words_A+words_B+1e-6))),\n",
        "        'B': (0.6*(dur_B/total_time) + 0.4*(words_B/(words_A+words_B+1e-6)))\n",
        "    }\n",
        "\n",
        "    # Improved question detection\n",
        "    num_questions = 0\n",
        "    question_pattern = re.compile(r'(\\?|\\b(what|why|how|when|where|who|which|can|could|would|should|do|did|does|is|are|am)\\b)', re.IGNORECASE)\n",
        "    for sp,st,ed,txt in turns:\n",
        "        if question_pattern.search(txt):\n",
        "            num_questions += 1\n",
        "\n",
        "    # Longest monologue (based on continuous speech duration)\n",
        "    longest_mono = 0.0\n",
        "    cur_speaker, cur_start, cur_end = None, None, None\n",
        "    for sp,st,ed,tx in turns:\n",
        "        if sp != cur_speaker:\n",
        "            if cur_speaker is not None:\n",
        "                longest_mono = max(longest_mono, cur_end - cur_start)\n",
        "            cur_speaker, cur_start, cur_end = sp, st, ed\n",
        "        else:\n",
        "            cur_end = ed\n",
        "    if cur_speaker is not None:\n",
        "        longest_mono = max(longest_mono, cur_end - cur_start)\n",
        "\n",
        "    # Sentiment (averaged across turns for robustness)\n",
        "    sentiment_pipe = pipeline('sentiment-analysis')\n",
        "    sentiments = [sentiment_pipe(tx[:512])[0]['label'] for sp,st,ed,tx in turns if tx]\n",
        "    pos = sentiments.count('POSITIVE')\n",
        "    neg = sentiments.count('NEGATIVE')\n",
        "    neu = len(sentiments) - pos - neg\n",
        "    if pos >= max(neg, neu):\n",
        "        call_sentiment = 'POSITIVE'\n",
        "    elif neg >= max(pos, neu):\n",
        "        call_sentiment = 'NEGATIVE'\n",
        "    else:\n",
        "        call_sentiment = 'NEUTRAL'\n",
        "\n",
        "    # Actionable insight (contextual)\n",
        "    if talk_ratio['A'] > 0.7 or talk_ratio['B'] > 0.7:\n",
        "        dominant = 'A' if talk_ratio['A']>talk_ratio['B'] else 'B'\n",
        "        insight = f\"Speaker {dominant} is dominating (~{talk_ratio[dominant]*100:.1f}%). Encourage the other speaker to participate more.\"\n",
        "    elif num_questions < 3:\n",
        "        insight = \"Few questions were asked. Recommend using more open-ended questions to engage the other party.\"\n",
        "    elif call_sentiment == 'NEGATIVE':\n",
        "        insight = \"Overall negative sentiment detected. Address objections proactively and ensure clarity of next steps.\"\n",
        "    else:\n",
        "        insight = \"Balanced interaction with healthy engagement. Consider summarizing agreed points and next steps at the end.\"\n",
        "\n",
        "    return {\n",
        "        'talk_time_ratio': talk_ratio,\n",
        "        'num_questions': num_questions,\n",
        "        'longest_monologue_s': longest_mono,\n",
        "        'call_sentiment': call_sentiment,\n",
        "        'insight': insight,\n",
        "        'turns': turns\n",
        "    }\n",
        "\n",
        "# %%%\n",
        "# 7) Run end-to-end\n",
        "print('Downloading audio...')\n",
        "download_audio(YOUTUBE_URL)\n",
        "print('Analyzing...')\n",
        "start = time.time()\n",
        "res = analyze_call(OUT_WAV)\n",
        "print('Done in', time.time()-start, 's')\n",
        "\n",
        "# %%%\n",
        "# 8) Print results\n",
        "print('\\n--- Results ---')\n",
        "print('Talk-time ratio (A/B):', res['talk_time_ratio'])\n",
        "print('Number of questions:', res['num_questions'])\n",
        "print('Longest monologue (s):', res['longest_monologue_s'])\n",
        "print('Call sentiment:', res['call_sentiment'])\n",
        "print('Actionable insight:', res['insight'])\n",
        "\n",
        "with open('turns.csv','w',newline='',encoding='utf-8') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerow(['speaker','start','end','text'])\n",
        "    for sp,st,ed,tx in res['turns']:\n",
        "        w.writerow([sp,st,ed,tx])\n",
        "print('Saved turns.csv')\n",
        "\n",
        "# %%%\n",
        "# Notes\n",
        "# - For robust speaker-diarization, integrate pyannote.audio\n",
        "# - For more accurate ASR, use faster-whisper or OpenAI API\n",
        "# - Tiny Whisper is chosen for speed to stay <30s\n",
        "\n",
        "# End of notebook"
      ]
    }
  ]
}